{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data - Trabalho Final - Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial configuration for Spark + JVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master = \"local[*]\"\n",
    "launcher.driver_memory = '20g'\n",
    "launcher.executor_memory = '20g'\n",
    "launcher.verbose = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spylon-kernel\n",
      "local[*]\n"
     ]
    }
   ],
   "source": [
    "println(sc.appName)\n",
    "println(sc.master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import spark.implicits._\n",
       "import org.apache.spark.sql.expressions._\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._      // include the Spark Types to define our schema\n",
    "import org.apache.spark.sql.functions._  // include the Spark helper functions\n",
    "import spark.implicits._                 // For implicit conversions like converting RDDs to DataFrames\n",
    "import org.apache.spark.sql.expressions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB JSON schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location_schema: org.apache.spark.sql.types.MapType = MapType(StringType,StructType(StructField(accuracy,DoubleType,true), StructField(address,StringType,true), StructField(altitude,DoubleType,true), StructField(country,StringType,true), StructField(latitude,DoubleType,true), StructField(longitude,DoubleType,true), StructField(provider,StringType,true), StructField(timestamp,StructType(StructField(date,LongType,true), StructField(day,LongType,true), StructField(hours,LongType,true), StructField(minutes,LongType,true), StructField(month,LongType,true), StructField(nanos,LongType,true), StructField(seconds,LongType,true), StructField(time,LongType,true), StructField(timezoneOffset,LongType,true), StructField(year,LongType,true)),true), StructField(uid,StringType,true)),true)\n",
       "schema: org.a...\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val location_schema =\n",
    "    MapType(StringType,\n",
    "        new StructType()\n",
    "            .add(\"accuracy\", DoubleType)\n",
    "            .add(\"address\", StringType)\n",
    "            .add(\"altitude\", DoubleType)\n",
    "            .add(\"country\", StringType)\n",
    "            .add(\"latitude\", DoubleType)\n",
    "            .add(\"longitude\", DoubleType)\n",
    "            .add(\"provider\", StringType)\n",
    "            .add(\"timestamp\", \n",
    "             new StructType()\n",
    "                .add(\"date\", LongType)\n",
    "                .add(\"day\", LongType)\n",
    "                .add(\"hours\", LongType)\n",
    "                .add(\"minutes\", LongType)\n",
    "                .add(\"month\", LongType)\n",
    "                .add(\"nanos\", LongType)\n",
    "                .add(\"seconds\", LongType)\n",
    "                .add(\"time\", LongType)\n",
    "                .add(\"timezoneOffset\", LongType)\n",
    "                .add(\"year\", LongType)\n",
    "            )\n",
    "            .add(\"uid\", StringType)\n",
    "        )\n",
    "\n",
    "val schema = new StructType()\n",
    "    .add(\"locations\", location_schema)\n",
    "    .add(\"user-locations\",\n",
    "        MapType(StringType, location_schema)\n",
    "    )\n",
    "    .add(\"users\",\n",
    "        MapType(StringType,\n",
    "            new StructType()\n",
    "                .add(\"email\", StringType)\n",
    "                .add(\"username\", StringType)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import JSON DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [locations: map<string,struct<accuracy:double,address:string,altitude:double,country:string,latitude:double,longitude:double,provider:string,timestamp:struct<date:bigint,day:bigint,hours:bigint,minutes:bigint,month:bigint,nanos:bigint,seconds:bigint,time:bigint,timezoneOffset:bigint,year:bigint>,uid:string>>, user-locations: map<string,map<string,struct<accuracy:double,address:string,altitude:double,country:string,latitude:double,longitude:double,provider:string,timestamp:struct<date:bigint,day:bigint,hours:bigint,minutes:bigint,month:bigint,nanos:bigint,seconds:bigint,time:bigint,timezoneOffset:bigint,year:bigint>,uid:string>>> ... 1 more field]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.option(\"multiline\", true).schema(schema).json(\"trackme-export.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- locations: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: struct (valueContainsNull = true)\n",
      " |    |    |-- accuracy: double (nullable = true)\n",
      " |    |    |-- address: string (nullable = true)\n",
      " |    |    |-- altitude: double (nullable = true)\n",
      " |    |    |-- country: string (nullable = true)\n",
      " |    |    |-- latitude: double (nullable = true)\n",
      " |    |    |-- longitude: double (nullable = true)\n",
      " |    |    |-- provider: string (nullable = true)\n",
      " |    |    |-- timestamp: struct (nullable = true)\n",
      " |    |    |    |-- date: long (nullable = true)\n",
      " |    |    |    |-- day: long (nullable = true)\n",
      " |    |    |    |-- hours: long (nullable = true)\n",
      " |    |    |    |-- minutes: long (nullable = true)\n",
      " |    |    |    |-- month: long (nullable = true)\n",
      " |    |    |    |-- nanos: long (nullable = true)\n",
      " |    |    |    |-- seconds: long (nullable = true)\n",
      " |    |    |    |-- time: long (nullable = true)\n",
      " |    |    |    |-- timezoneOffset: long (nullable = true)\n",
      " |    |    |    |-- year: long (nullable = true)\n",
      " |    |    |-- uid: string (nullable = true)\n",
      " |-- user-locations: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: map (valueContainsNull = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: struct (valueContainsNull = true)\n",
      " |    |    |    |-- accuracy: double (nullable = true)\n",
      " |    |    |    |-- address: string (nullable = true)\n",
      " |    |    |    |-- altitude: double (nullable = true)\n",
      " |    |    |    |-- country: string (nullable = true)\n",
      " |    |    |    |-- latitude: double (nullable = true)\n",
      " |    |    |    |-- longitude: double (nullable = true)\n",
      " |    |    |    |-- provider: string (nullable = true)\n",
      " |    |    |    |-- timestamp: struct (nullable = true)\n",
      " |    |    |    |    |-- date: long (nullable = true)\n",
      " |    |    |    |    |-- day: long (nullable = true)\n",
      " |    |    |    |    |-- hours: long (nullable = true)\n",
      " |    |    |    |    |-- minutes: long (nullable = true)\n",
      " |    |    |    |    |-- month: long (nullable = true)\n",
      " |    |    |    |    |-- nanos: long (nullable = true)\n",
      " |    |    |    |    |-- seconds: long (nullable = true)\n",
      " |    |    |    |    |-- time: long (nullable = true)\n",
      " |    |    |    |    |-- timezoneOffset: long (nullable = true)\n",
      " |    |    |    |    |-- year: long (nullable = true)\n",
      " |    |    |    |-- uid: string (nullable = true)\n",
      " |-- users: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: struct (valueContainsNull = true)\n",
      " |    |    |-- email: string (nullable = true)\n",
      " |    |    |-- username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separação do banco em DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawLocationsDF: org.apache.spark.sql.DataFrame = [timestamp_id: string, value: struct<accuracy: double, address: string ... 7 more fields>]\n",
       "rawUserLocationsDF: org.apache.spark.sql.DataFrame = [uid: string, timestamp: map<string,struct<accuracy:double,address:string,altitude:double,country:string,latitude:double,longitude:double,provider:string,timestamp:struct<date:bigint,day:bigint,hours:bigint,minutes:bigint,month:bigint,nanos:bigint,seconds:bigint,time:bigint,timezoneOffset:bigint,year:bigint>,uid:string>>]\n",
       "rawUsersDF: org.apache.spark.sql.DataFrame = [uid: string, user_attr: struct<email: string, username: string>]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawLocationsDF = df.select(explode($\"locations\") as Seq(\"timestamp_id\", \"value\"))\n",
    "val rawUserLocationsDF = df.select(explode($\"user-locations\") as Seq(\"uid\", \"timestamp\"))\n",
    "val rawUsersDF = df.select(explode($\"users\") as Seq(\"uid\", \"user_attr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = false)\n",
      " |-- user_attr: struct (nullable = true)\n",
      " |    |-- email: string (nullable = true)\n",
      " |    |-- username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawUsersDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função para \"flatenizar\" o schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Column\n",
       "flattenSchema: (schema: org.apache.spark.sql.types.StructType, prefix: String)Array[org.apache.spark.sql.Column]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Column\n",
    "\n",
    "def flattenSchema(schema: StructType, prefix: String = null) : Array[Column] = {\n",
    "  schema.fields.flatMap(f => {\n",
    "    val colName = if (prefix == null) f.name else (prefix + \".\" + f.name)\n",
    "\n",
    "    f.dataType match {\n",
    "      case st: StructType => flattenSchema(st, colName)\n",
    "      case _ => Array(col(colName))\n",
    "    }\n",
    "  })\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso da função nos DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "locationsDF: org.apache.spark.sql.DataFrame = [timestamp_id: string, accuracy: double ... 17 more fields]\n",
       "userLocationsDF: org.apache.spark.sql.DataFrame = [uid: string, timestamp: map<string,struct<accuracy:double,address:string,altitude:double,country:string,latitude:double,longitude:double,provider:string,timestamp:struct<date:bigint,day:bigint,hours:bigint,minutes:bigint,month:bigint,nanos:bigint,seconds:bigint,time:bigint,timezoneOffset:bigint,year:bigint>,uid:string>>]\n",
       "usersDF: org.apache.spark.sql.DataFrame = [uid: string, email: string ... 1 more field]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val locationsDF = rawLocationsDF.select(flattenSchema(rawLocationsDF.schema):_*)\n",
    "val userLocationsDF = rawUserLocationsDF.select(flattenSchema(rawUserLocationsDF.schema):_*)\n",
    "val usersDF = rawUsersDF.select(flattenSchema(rawUsersDF.schema):_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp_id: string (nullable = false)\n",
      " |-- accuracy: double (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- altitude: double (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- provider: string (nullable = true)\n",
      " |-- date: long (nullable = true)\n",
      " |-- day: long (nullable = true)\n",
      " |-- hours: long (nullable = true)\n",
      " |-- minutes: long (nullable = true)\n",
      " |-- month: long (nullable = true)\n",
      " |-- nanos: long (nullable = true)\n",
      " |-- seconds: long (nullable = true)\n",
      " |-- time: long (nullable = true)\n",
      " |-- timezoneOffset: long (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "locationsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5Jf44SGWhzZmxsZs7n6KLzrHark1,rodrigomesquita0@gmail.com,rodrigomesquita0]\n",
      "[BHNpkg1LH2Sna0axjb8pFWDIycD2,vivian.lopesg@gmail.com,vivian.lopesg]\n"
     ]
    }
   ],
   "source": [
    "usersDF.take(2).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar coluna com tipo Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "locationsWithDateDF: org.apache.spark.sql.DataFrame = [timestamp_id: string, accuracy: double ... 18 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val locationsWithDateDF = locationsDF.withColumn(\"ts_date\", (col(\"timestamp_id\")/1000).cast(TimestampType))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join com Users DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joinExpression: org.apache.spark.sql.Column = (uid = uid)\n",
       "joinType: String = inner\n",
       "locWithDateJoinUserDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [timestamp_id: string, accuracy: double ... 20 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joinExpression = locationsWithDateDF.col(\"uid\") === usersDF.col(\"uid\")\n",
    "var joinType = \"inner\"\n",
    "val locWithDateJoinUserDF = locationsWithDateDF.join(usersDF, joinExpression, joinType).drop(usersDF.col(\"uid\")).orderBy($\"timestamp_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[henrique.mageste@gmail.com,-22.9555958,-43.1880177,2020-10-07 22:52:37.435]\n",
      "[wallace.mendes.rj@gmail.com,-22.8381043,-43.2622822,2020-10-07 23:10:06.52]\n",
      "[viniciusmgaspar@gmail.com,-22.914351,-43.247925,2020-10-07 23:40:02.011]\n",
      "[wallace.mendes.rj@gmail.com,-22.8381148,-43.2623173,2020-10-08 00:21:00.085]\n",
      "[wallace.mendes.rj@gmail.com,-22.8381131,-43.2623054,2020-10-08 00:42:46.199]\n"
     ]
    }
   ],
   "source": [
    "locWithDateJoinUserDF.select(\"email\", \"latitude\", \"longitude\", \"ts_date\").sample(false, 0.04).take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função para calcular distância"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.math.Pi\n",
       "import org.apache.spark.sql.functions._\n",
       "calculate_distance_col: (lat1: org.apache.spark.sql.Column, lon1: org.apache.spark.sql.Column, lat2: org.apache.spark.sql.Column, lon2: org.apache.spark.sql.Column)org.apache.spark.sql.Column\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.math.Pi\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "def calculate_distance_col(lat1:org.apache.spark.sql.Column, lon1:org.apache.spark.sql.Column, lat2:org.apache.spark.sql.Column, lon2:org.apache.spark.sql.Column):org.apache.spark.sql.Column = {   \n",
    "    val earth_radius = 6371e3;           // meters\n",
    "    val pi_over_180 = lit(Pi/180);\n",
    "    val phi1 = lat1 * pi_over_180;                  // radians\n",
    "    val phi2 = lat2 * pi_over_180;                  // radians\n",
    "    val delta_phi = phi2 - phi1;               // radians\n",
    "\n",
    "    val delta_lampda = (lon2 - lon1) * pi_over_180; // radians\n",
    "\n",
    "    val a = sin(delta_phi/2)*sin(delta_phi/2) + cos(phi1)*cos(phi2)*sin(delta_lampda/2)*sin(delta_lampda/2);\n",
    "    val c = lit(2)*atan2(sqrt(a), sqrt(lit(1)-a));\n",
    "\n",
    "    val d = lit(earth_radius)*c; // meters\n",
    "    \n",
    "    return d;\n",
    "}\n",
    "\n",
    "// val calculate_distance_sqlfunc = udf(calculate_distance(_,_,_,_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo Distância"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lat_col: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@3f3135d8\n",
       "lon_col: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@5b45f3bf\n",
       "lat2: org.apache.spark.sql.Column = latitude\n",
       "lat1: org.apache.spark.sql.Column = lag(latitude, 1, NULL) OVER (ORDER BY timestamp_id ASC NULLS FIRST unspecifiedframe$())\n",
       "lon2: org.apache.spark.sql.Column = longitude\n",
       "lon1: org.apache.spark.sql.Column = lag(longitude, 1, NULL) OVER (ORDER BY timestamp_id ASC NULLS FIRST unspecifiedframe$())\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val lat_col = Window.partitionBy(\"latitude\").orderBy($\"timestamp_id\".asc)\n",
    "// val lon_col = Window.partitionBy(\"longitude\").orderBy($\"timestamp_id\".asc)\n",
    "val lat_col = Window.orderBy($\"timestamp_id\".asc)\n",
    "val lon_col = Window.orderBy($\"timestamp_id\".asc)\n",
    "\n",
    "val lat2 = col(\"latitude\")\n",
    "val lat1 = lag(\"latitude\", 1).over(lat_col)\n",
    "// val lat1 = when((lag(\"latitude\", 1).over(lat_col)).isNotNull, lag(\"latitude\", 1).over(lat_col)).otherwise(0)\n",
    "\n",
    "val lon2 = col(\"longitude\")\n",
    "val lon1 = lag(\"longitude\", 1).over(lat_col)\n",
    "// val lon1 = when((lag(\"longitude\", 1).over(lon_col)).isNotNull, lag(\"longitude\", 1).over(lon_col)).otherwise(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emails: String = viniciusmgaspar@gmail.com\n",
       "joinExpression: org.apache.spark.sql.Column = (uid = uid)\n",
       "joinType: String = inner\n",
       "locationsWithDatePerUserDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [timestamp_id: string, accuracy: double ... 20 more fields]\n",
       "locDatePerUserDistDF: org.apache.spark.sql.DataFrame = [timestamp_id: string, accuracy: double ... 21 more fields]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val emails = \"viniciusmgaspar@gmail.com\"\n",
    "\n",
    "val joinExpression = locationsWithDateDF.col(\"uid\") === usersDF.col(\"uid\")\n",
    "var joinType = \"inner\"\n",
    "val locationsWithDatePerUserDF = locationsWithDateDF.join(usersDF, joinExpression, joinType).drop(usersDF.col(\"uid\")).filter($\"Email\" === emails).orderBy($\"timestamp_id\")\n",
    "\n",
    "val locDatePerUserDistDF = locationsWithDatePerUserDF.withColumn(\"distance\", when(calculate_distance_col(lat1, lon1, lat2, lon2).isNotNull,calculate_distance_col(lat1, lon1, lat2, lon2)).otherwise(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mostrar distância"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------+-----------+-----------+------------------+\n",
      "|username       |ts_date                |latitude   |longitude  |distance          |\n",
      "+---------------+-----------------------+-----------+-----------+------------------+\n",
      "|viniciusmgaspar|2020-10-07 23:32:51.945|-22.9143424|-43.2479267|0.0               |\n",
      "|viniciusmgaspar|2020-10-07 23:34:51.961|-22.9143476|-43.2479225|0.720675492840136 |\n",
      "|viniciusmgaspar|2020-10-07 23:36:56.689|-22.9143736|-43.2479302|2.9967018357873862|\n",
      "|viniciusmgaspar|2020-10-07 23:37:57.042|-22.9143736|-43.2479302|0.0               |\n",
      "|viniciusmgaspar|2020-10-07 23:38:57.116|-22.9143736|-43.2479302|0.0               |\n",
      "|viniciusmgaspar|2020-10-07 23:40:02.011|-22.914351 |-43.247925 |2.5688213633650445|\n",
      "|viniciusmgaspar|2020-10-07 23:41:02.089|-22.914351 |-43.247925 |0.0               |\n",
      "|viniciusmgaspar|2020-10-07 23:42:09.072|-22.9143672|-43.2479284|1.8347079899956038|\n",
      "|viniciusmgaspar|2020-10-07 23:43:09.149|-22.9143672|-43.2479284|0.0               |\n",
      "|viniciusmgaspar|2020-10-07 23:44:10.298|-22.9143608|-43.2479235|0.8708071547226146|\n",
      "|viniciusmgaspar|2020-10-07 23:45:10.37 |-22.9143608|-43.2479235|0.0               |\n",
      "|viniciusmgaspar|2020-10-07 23:46:20.77 |-22.9143608|-43.2479235|0.0               |\n",
      "|viniciusmgaspar|2020-10-07 23:47:20.86 |-22.9143608|-43.2479235|0.0               |\n",
      "|viniciusmgaspar|2020-10-07 23:48:26.977|-22.9143612|-43.2479639|4.138018973396575 |\n",
      "|viniciusmgaspar|2020-10-07 23:49:33.323|-22.914376 |-43.2479024|6.510281292123442 |\n",
      "|viniciusmgaspar|2020-10-07 23:50:33.676|-22.914376 |-43.2479024|0.0               |\n",
      "|viniciusmgaspar|2020-10-07 23:51:33.654|-22.9143854|-43.2478801|2.5117802113336802|\n",
      "|viniciusmgaspar|2020-10-07 23:52:33.892|-22.9143854|-43.2478801|0.0               |\n",
      "|viniciusmgaspar|2020-10-07 23:53:33.964|-22.9143854|-43.2478801|0.0               |\n",
      "|viniciusmgaspar|2020-10-07 23:54:34.786|-22.9143797|-43.2478934|1.5024238063887392|\n",
      "+---------------+-----------------------+-----------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "locDatePerUserDistDF.orderBy($\"ts_date\".asc)\n",
    ".select(\"username\",\"ts_date\",\"latitude\",\"longitude\",\"distance\")\n",
    "    .show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distância por dia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+----------+------------------+\n",
      "|email                    |date      |sum(distance)     |\n",
      "+-------------------------+----------+------------------+\n",
      "|viniciusmgaspar@gmail.com|07-10-2020|27.519468577232455|\n",
      "|viniciusmgaspar@gmail.com|08-10-2020|27236.48613099252 |\n",
      "|viniciusmgaspar@gmail.com|09-10-2020|9797.745650086414 |\n",
      "|viniciusmgaspar@gmail.com|11-10-2020|617.8502728067592 |\n",
      "|viniciusmgaspar@gmail.com|12-10-2020|2232.8849177640095|\n",
      "+-------------------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "locDatePerUserDistDF.groupBy($\"email\",date_format(col(\"ts_date\"),\"dd-MM-yyyy\").as(\"date\")).sum(\"distance\").orderBy($\"email\".asc, $\"date\".asc)\n",
    ".show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distância entre duas pessoas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user1: String = henrique.mageste@gmail.com\n",
       "user2: String = wallace.mendes.rj@gmail.com\n",
       "user1_DF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [timestamp_id: string, accuracy: double ... 21 more fields]\n",
       "user2_DF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [timestamp_id: string, accuracy: double ... 21 more fields]\n",
       "joinExpression: org.apache.spark.sql.Column = (formatted_date = formatted_date)\n",
       "joinType: String = inner\n",
       "c: org.apache.spark.sql.DataFrame = [timestamp_id: string, accuracy: double ... 44 more fields]\n",
       "c2: org.apache.spark.sql.DataFrame = [formatted_date: string, formatted_date: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val user1 = \"henrique.mageste@gmail.com\"\n",
    "val user2 = \"wallace.mendes.rj@gmail.com\"\n",
    "\n",
    "val user1_DF = locWithDateJoinUserDF.withColumn(\"formatted_date\", date_format($\"ts_date\",\"dd-MM-yyyy HH:mm\")).filter($\"email\" === user1).as(\"user1\")\n",
    "val user2_DF = locWithDateJoinUserDF.withColumn(\"formatted_date\", date_format($\"ts_date\",\"dd-MM-yyyy HH:mm\")).filter($\"email\" === user2).as(\"user2\")\n",
    "\n",
    "val joinExpression = ( user1_DF.col(\"formatted_date\") === user2_DF.col(\"formatted_date\"))\n",
    "val joinType = \"inner\"\n",
    "val c = user1_DF.join(user2_DF, joinExpression, joinType)\n",
    "    \n",
    "val c2= c.select(\n",
    "    col(\"user1.formatted_date\")\n",
    "    ,col(\"user2.formatted_date\")\n",
    "    ,col(\"user1.email\")\n",
    "    ,col(\"user2.email\")\n",
    "    ,col(\"user1.longitude\")\n",
    "    ,col(\"user2.longitude\")\n",
    "    ,col(\"user1.latitude\")\n",
    "    ,col(\"user2.latitude\")\n",
    "    ,col(\"user1.address\")\n",
    "    ,col(\"user2.address\")\n",
    "    )\n",
    "    .withColumn(\"distance_between\", calculate_distance_col(col(\"user1.latitude\"), col(\"user1.longitude\"), col(\"user2.latitude\"), col(\"user2.longitude\")) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+---------------------+---------------------+\n",
      "|formatted_date  |formatted_date  |min(distance_between)|max(distance_between)|\n",
      "+----------------+----------------+---------------------+---------------------+\n",
      "|07-10-2020 22:49|07-10-2020 22:49|15138.752694453176   |15138.752694453176   |\n",
      "|07-10-2020 22:50|07-10-2020 22:50|15367.886012191113   |15367.886012191113   |\n",
      "|07-10-2020 22:52|07-10-2020 22:52|15111.625491525283   |15111.625491525283   |\n",
      "|07-10-2020 22:54|07-10-2020 22:54|15117.047889628033   |15119.267698828335   |\n",
      "|07-10-2020 22:57|07-10-2020 22:57|15118.077374728518   |15118.077374728518   |\n",
      "|07-10-2020 22:59|07-10-2020 22:59|15118.90405992265    |15121.055502987601   |\n",
      "|07-10-2020 23:00|07-10-2020 23:00|15122.087961512667   |15122.087961512667   |\n",
      "|07-10-2020 23:02|07-10-2020 23:02|15122.191299320195   |15122.191299320195   |\n",
      "|07-10-2020 23:03|07-10-2020 23:03|15122.539946497187   |15122.539946497187   |\n",
      "|07-10-2020 23:04|07-10-2020 23:04|15122.702272879895   |15122.702272879895   |\n",
      "|07-10-2020 23:05|07-10-2020 23:05|15122.514270273936   |15122.514270273936   |\n",
      "|07-10-2020 23:18|07-10-2020 23:18|15122.676462968528   |15122.676462968528   |\n",
      "|07-10-2020 23:21|07-10-2020 23:21|15128.676115410955   |15128.676115410955   |\n",
      "|07-10-2020 23:24|07-10-2020 23:24|15122.710763217514   |15122.710763217514   |\n",
      "|08-10-2020 00:16|08-10-2020 00:16|15121.754385878157   |15213.285690905239   |\n",
      "|08-10-2020 00:18|08-10-2020 00:18|15121.878406938411   |15121.878406938411   |\n",
      "|08-10-2020 00:22|08-10-2020 00:22|15122.676179017539   |15122.676179017539   |\n",
      "|08-10-2020 00:23|08-10-2020 00:23|15117.668511047192   |15117.668511047192   |\n",
      "|08-10-2020 00:24|08-10-2020 00:24|15119.741341112529   |15119.741341112529   |\n",
      "|08-10-2020 00:26|08-10-2020 00:26|15120.380487749486   |15120.380487749486   |\n",
      "+----------------+----------------+---------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------------+---------------------------+---------------------+---------------------+\n",
      "|email                     |email                      |min(distance_between)|max(distance_between)|\n",
      "+--------------------------+---------------------------+---------------------+---------------------+\n",
      "|henrique.mageste@gmail.com|wallace.mendes.rj@gmail.com|15003.316084810856   |15367.886012191113   |\n",
      "+--------------------------+---------------------------+---------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// AGREGADO POR DATA\n",
    "c2.groupBy(col(\"user1.formatted_date\"),col(\"user2.formatted_date\")).agg(min(\"distance_between\"),max(\"distance_between\")).orderBy($\"user1.formatted_date\").show(false)\n",
    "\n",
    "// AGREGADO POR USUÁRIO    \n",
    "c2.groupBy(col(\"user1.email\"),col(\"user2.email\")).agg(min(\"distance_between\"),max(\"distance_between\")).show(false)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
