{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data - Final Work - Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial configuration for Spark + JVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master = \"local[*]\"\n",
    "launcher.driver_memory = '20g'\n",
    "launcher.executor_memory = '20g'\n",
    "launcher.verbose = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.conf.set(\"spark.app.name\", \"scalaXgbTest\")\n",
    "launcher.num_executors = 3\n",
    "launcher.executor_cores = 7 //launcher.conf.spark.executor.cores = 8\n",
    "launcher.conf.spark.task.cpus = 6\n",
    "launcher.conf.set(\"spark.executor.heartbeatInterval\", \"6000s\")\n",
    "launcher.conf.set(\"spark.yarn.scheduler.heartbeat.interval-ms\", \"10000s\")\n",
    "launcher.conf.set(\"spark.network.timeout\", \"10000s\")\n",
    "launcher.conf.set(\"spark.yarn.executor.memoryOverhead\", \"8192\")\n",
    "launcher.conf.set(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "launcher.jars = [\"file://some/jar.jar\", \"xgboost-maven-0.82/xgboost4j-spark-0.82.jar\", \"xgboost-maven-0.82/xgboost4j-0.82.jar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spylon-kernel\n",
      "local[*]\n"
     ]
    }
   ],
   "source": [
    "println(sc.appName)\n",
    "println(sc.master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import spark.implicits._\n",
       "import org.apache.spark.sql.expressions._\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._      // include the Spark Types to define our schema\n",
    "import org.apache.spark.sql.functions._  // include the Spark helper functions\n",
    "import spark.implicits._                 // For implicit conversions like converting RDDs to DataFrames\n",
    "import org.apache.spark.sql.expressions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB JSON schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location_schema: org.apache.spark.sql.types.MapType = MapType(StringType,StructType(StructField(accuracy,DoubleType,true), StructField(address,StringType,true), StructField(altitude,DoubleType,true), StructField(country,StringType,true), StructField(latitude,DoubleType,true), StructField(longitude,DoubleType,true), StructField(provider,StringType,true), StructField(timestamp,StructType(StructField(date,LongType,true), StructField(day,LongType,true), StructField(hours,LongType,true), StructField(minutes,LongType,true), StructField(month,LongType,true), StructField(nanos,LongType,true), StructField(seconds,LongType,true), StructField(time,LongType,true), StructField(timezoneOffset,LongType,true), StructField(year,LongType,true)),true), StructField(uid,StringType,true)),true)\n",
       "schema: org.a...\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val location_schema =\n",
    "    MapType(StringType,\n",
    "        new StructType()\n",
    "            .add(\"accuracy\", DoubleType)\n",
    "            .add(\"address\", StringType)\n",
    "            .add(\"altitude\", DoubleType)\n",
    "            .add(\"country\", StringType)\n",
    "            .add(\"latitude\", DoubleType)\n",
    "            .add(\"longitude\", DoubleType)\n",
    "            .add(\"provider\", StringType)\n",
    "            .add(\"timestamp\", \n",
    "             new StructType()\n",
    "                .add(\"date\", LongType)\n",
    "                .add(\"day\", LongType)\n",
    "                .add(\"hours\", LongType)\n",
    "                .add(\"minutes\", LongType)\n",
    "                .add(\"month\", LongType)\n",
    "                .add(\"nanos\", LongType)\n",
    "                .add(\"seconds\", LongType)\n",
    "                .add(\"time\", LongType)\n",
    "                .add(\"timezoneOffset\", LongType)\n",
    "                .add(\"year\", LongType)\n",
    "            )\n",
    "            .add(\"uid\", StringType)\n",
    "        )\n",
    "\n",
    "val schema = new StructType()\n",
    "    .add(\"locations\", location_schema)\n",
    "    .add(\"user-locations\",\n",
    "        MapType(StringType, location_schema)\n",
    "    )\n",
    "    .add(\"users\",\n",
    "        MapType(StringType,\n",
    "            new StructType()\n",
    "                .add(\"email\", StringType)\n",
    "                .add(\"username\", StringType)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import JSON DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [locations: map<string,struct<accuracy:double,address:string,altitude:double,country:string,latitude:double,longitude:double,provider:string,timestamp:struct<date:bigint,day:bigint,hours:bigint,minutes:bigint,month:bigint,nanos:bigint,seconds:bigint,time:bigint,timezoneOffset:bigint,year:bigint>,uid:string>>, user-locations: map<string,map<string,struct<accuracy:double,address:string,altitude:double,country:string,latitude:double,longitude:double,provider:string,timestamp:struct<date:bigint,day:bigint,hours:bigint,minutes:bigint,month:bigint,nanos:bigint,seconds:bigint,time:bigint,timezoneOffset:bigint,year:bigint>,uid:string>>> ... 1 more field]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.option(\"multiline\", true).schema(schema).json(\"trackme-export.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- locations: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: struct (valueContainsNull = true)\n",
      " |    |    |-- accuracy: double (nullable = true)\n",
      " |    |    |-- address: string (nullable = true)\n",
      " |    |    |-- altitude: double (nullable = true)\n",
      " |    |    |-- country: string (nullable = true)\n",
      " |    |    |-- latitude: double (nullable = true)\n",
      " |    |    |-- longitude: double (nullable = true)\n",
      " |    |    |-- provider: string (nullable = true)\n",
      " |    |    |-- timestamp: struct (nullable = true)\n",
      " |    |    |    |-- date: long (nullable = true)\n",
      " |    |    |    |-- day: long (nullable = true)\n",
      " |    |    |    |-- hours: long (nullable = true)\n",
      " |    |    |    |-- minutes: long (nullable = true)\n",
      " |    |    |    |-- month: long (nullable = true)\n",
      " |    |    |    |-- nanos: long (nullable = true)\n",
      " |    |    |    |-- seconds: long (nullable = true)\n",
      " |    |    |    |-- time: long (nullable = true)\n",
      " |    |    |    |-- timezoneOffset: long (nullable = true)\n",
      " |    |    |    |-- year: long (nullable = true)\n",
      " |    |    |-- uid: string (nullable = true)\n",
      " |-- user-locations: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: map (valueContainsNull = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: struct (valueContainsNull = true)\n",
      " |    |    |    |-- accuracy: double (nullable = true)\n",
      " |    |    |    |-- address: string (nullable = true)\n",
      " |    |    |    |-- altitude: double (nullable = true)\n",
      " |    |    |    |-- country: string (nullable = true)\n",
      " |    |    |    |-- latitude: double (nullable = true)\n",
      " |    |    |    |-- longitude: double (nullable = true)\n",
      " |    |    |    |-- provider: string (nullable = true)\n",
      " |    |    |    |-- timestamp: struct (nullable = true)\n",
      " |    |    |    |    |-- date: long (nullable = true)\n",
      " |    |    |    |    |-- day: long (nullable = true)\n",
      " |    |    |    |    |-- hours: long (nullable = true)\n",
      " |    |    |    |    |-- minutes: long (nullable = true)\n",
      " |    |    |    |    |-- month: long (nullable = true)\n",
      " |    |    |    |    |-- nanos: long (nullable = true)\n",
      " |    |    |    |    |-- seconds: long (nullable = true)\n",
      " |    |    |    |    |-- time: long (nullable = true)\n",
      " |    |    |    |    |-- timezoneOffset: long (nullable = true)\n",
      " |    |    |    |    |-- year: long (nullable = true)\n",
      " |    |    |    |-- uid: string (nullable = true)\n",
      " |-- users: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: struct (valueContainsNull = true)\n",
      " |    |    |-- email: string (nullable = true)\n",
      " |    |    |-- username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown raw DB into contextual structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawLocationsDF: org.apache.spark.sql.DataFrame = [timestamp_id: string, value: struct<accuracy: double, address: string ... 7 more fields>]\n",
       "rawUserLocationsDF: org.apache.spark.sql.DataFrame = [uid: string, timestamp: map<string,struct<accuracy:double,address:string,altitude:double,country:string,latitude:double,longitude:double,provider:string,timestamp:struct<date:bigint,day:bigint,hours:bigint,minutes:bigint,month:bigint,nanos:bigint,seconds:bigint,time:bigint,timezoneOffset:bigint,year:bigint>,uid:string>>]\n",
       "rawUsersDF: org.apache.spark.sql.DataFrame = [uid: string, user_attr: struct<email: string, username: string>]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawLocationsDF = df.select(explode($\"locations\") as Seq(\"timestamp_id\", \"value\"))\n",
    "val rawUserLocationsDF = df.select(explode($\"user-locations\") as Seq(\"uid\", \"timestamp\"))\n",
    "val rawUsersDF = df.select(explode($\"users\") as Seq(\"uid\", \"user_attr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = false)\n",
      " |-- user_attr: struct (nullable = true)\n",
      " |    |-- email: string (nullable = true)\n",
      " |    |-- username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawUsersDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to flatten schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Column\n",
       "flattenSchema: (schema: org.apache.spark.sql.types.StructType, prefix: String)Array[org.apache.spark.sql.Column]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Column\n",
    "\n",
    "def flattenSchema(schema: StructType, prefix: String = null) : Array[Column] = {\n",
    "  schema.fields.flatMap(f => {\n",
    "    val colName = if (prefix == null) f.name else (prefix + \".\" + f.name)\n",
    "\n",
    "    f.dataType match {\n",
    "      case st: StructType => flattenSchema(st, colName)\n",
    "      case _ => Array(col(colName))\n",
    "    }\n",
    "  })\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flat struct DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "locationsDF: org.apache.spark.sql.DataFrame = [timestamp_id: string, accuracy: double ... 17 more fields]\n",
       "userLocationsDF: org.apache.spark.sql.DataFrame = [uid: string, timestamp: map<string,struct<accuracy:double,address:string,altitude:double,country:string,latitude:double,longitude:double,provider:string,timestamp:struct<date:bigint,day:bigint,hours:bigint,minutes:bigint,month:bigint,nanos:bigint,seconds:bigint,time:bigint,timezoneOffset:bigint,year:bigint>,uid:string>>]\n",
       "usersDF: org.apache.spark.sql.DataFrame = [uid: string, email: string ... 1 more field]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val locationsDF = rawLocationsDF.select(flattenSchema(rawLocationsDF.schema):_*)\n",
    "val userLocationsDF = rawUserLocationsDF.select(flattenSchema(rawUserLocationsDF.schema):_*)\n",
    "val usersDF = rawUsersDF.select(flattenSchema(rawUsersDF.schema):_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp_id: string (nullable = false)\n",
      " |-- accuracy: double (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- altitude: double (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- provider: string (nullable = true)\n",
      " |-- date: long (nullable = true)\n",
      " |-- day: long (nullable = true)\n",
      " |-- hours: long (nullable = true)\n",
      " |-- minutes: long (nullable = true)\n",
      " |-- month: long (nullable = true)\n",
      " |-- nanos: long (nullable = true)\n",
      " |-- seconds: long (nullable = true)\n",
      " |-- time: long (nullable = true)\n",
      " |-- timezoneOffset: long (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "locationsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5Jf44SGWhzZmxsZs7n6KLzrHark1,rodrigomesquita0@gmail.com,rodrigomesquita0]\n",
      "[BHNpkg1LH2Sna0axjb8pFWDIycD2,vivian.lopesg@gmail.com,vivian.lopesg]\n"
     ]
    }
   ],
   "source": [
    "usersDF.take(2).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create timestamp formatted column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "locationsWithDateDF: org.apache.spark.sql.DataFrame = [timestamp_id: string, accuracy: double ... 18 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val locationsWithDateDF = locationsDF.withColumn(\"ts_date\", (col(\"timestamp_id\")/1000).cast(TimestampType))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join with Users DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joinExpression: org.apache.spark.sql.Column = (uid = uid)\n",
       "joinType: String = inner\n",
       "locWithDateJoinUserDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [timestamp_id: string, accuracy: double ... 20 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joinExpression = locationsWithDateDF.col(\"uid\") === usersDF.col(\"uid\")\n",
    "var joinType = \"inner\"\n",
    "val locWithDateJoinUserDF = locationsWithDateDF.join(usersDF, joinExpression, joinType).drop(usersDF.col(\"uid\")).orderBy($\"timestamp_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wallace.mendes.rj@gmail.com,-22.8381126,-43.2623022,2020-10-07 23:00:28.694]\n",
      "[wallace.mendes.rj@gmail.com,-22.8381171,-43.2623166,2020-10-07 23:13:12.297]\n",
      "[wallace.mendes.rj@gmail.com,-22.838115,-43.2623124,2020-10-07 23:39:32.518]\n",
      "[viniciusmgaspar@gmail.com,-22.9143672,-43.2479284,2020-10-07 23:42:09.072]\n",
      "[viniciusmgaspar@gmail.com,-22.9143672,-43.2479284,2020-10-07 23:43:09.149]\n"
     ]
    }
   ],
   "source": [
    "locWithDateJoinUserDF.select(\"email\", \"latitude\", \"longitude\", \"ts_date\").sample(false, 0.04).take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for distance calculation based on lat/long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Element version - calculate_distance_elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.math._\n",
       "calculate_distance_elem: (lat1: Double, lon1: Double, lat2: Double, lon2: Double)Double\n",
       "calculate_distance_elem_sqlfunc: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$3841/1820714411@38347476,DoubleType,List(Some(class[value[0]: double]), Some(class[value[0]: double]), Some(class[value[0]: double]), Some(class[value[0]: double])),None,false,true)\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.math._\n",
    "\n",
    "def calculate_distance_elem(lat1:Double, lon1:Double, lat2:Double, lon2:Double):Double = {   \n",
    "    val earth_radius = 6371e3;           // meters\n",
    "    val phi1 = lat1 * Pi/180;                  // radians\n",
    "    val phi2 = lat2 * Pi/180;                  // radians\n",
    "    val delta_phi = phi2 - phi1;               // radians\n",
    "\n",
    "    val delta_lampda = (lon2 - lon1) * Pi/180; // radians\n",
    "\n",
    "    val a = sin(delta_phi/2)*sin(delta_phi/2) + cos(phi1)*cos(phi2)*sin(delta_lampda/2)*sin(delta_lampda/2);\n",
    "    val c = 2*atan2(sqrt(a), sqrt(1-a));\n",
    "\n",
    "    val d = earth_radius*c; // meters\n",
    "    \n",
    "    return d\n",
    "}\n",
    "\n",
    "val calculate_distance_elem_sqlfunc = udf(calculate_distance_elem(_,_,_,_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test - calculate_distance_elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val lat1 = -22.9556473\n",
    "val lon1 = -43.1881019\n",
    "\n",
    "val lat2 = -23.9556473\n",
    "val lon2 = -44.1881019\n",
    "\n",
    "val dist = calculate_distance_elem(lat1, lon1, lat2, lon2)\n",
    "\n",
    "assert (dist == 150894.75616346067)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.math.Pi\n",
       "import org.apache.spark.sql.functions._\n",
       "calculate_distance_col: (lat1: org.apache.spark.sql.Column, lon1: org.apache.spark.sql.Column, lat2: org.apache.spark.sql.Column, lon2: org.apache.spark.sql.Column)org.apache.spark.sql.Column\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.math.Pi\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "def calculate_distance_col(lat1:org.apache.spark.sql.Column, lon1:org.apache.spark.sql.Column, lat2:org.apache.spark.sql.Column, lon2:org.apache.spark.sql.Column):org.apache.spark.sql.Column = {   \n",
    "    val earth_radius = 6371e3;           // meters\n",
    "    val pi_over_180 = lit(Pi/180);\n",
    "    val phi1 = lat1 * pi_over_180;                  // radians\n",
    "    val phi2 = lat2 * pi_over_180;                  // radians\n",
    "    val delta_phi = phi2 - phi1;               // radians\n",
    "\n",
    "    val delta_lampda = (lon2 - lon1) * pi_over_180; // radians\n",
    "\n",
    "    val a = sin(delta_phi/2)*sin(delta_phi/2) + cos(phi1)*cos(phi2)*sin(delta_lampda/2)*sin(delta_lampda/2);\n",
    "    val c = lit(2)*atan2(sqrt(a), sqrt(lit(1)-a));\n",
    "\n",
    "    val d = lit(earth_radius)*c; // meters\n",
    "    \n",
    "    return d;\n",
    "}\n",
    "\n",
    "// val calculate_distance_sqlfunc = udf(calculate_distance(_,_,_,_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lat_col: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@65ebf5ba\n",
       "lon_col: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@7294fe7e\n",
       "lat2: org.apache.spark.sql.Column = latitude\n",
       "lat1: org.apache.spark.sql.Column = lag(latitude, 1, NULL) OVER (ORDER BY timestamp_id ASC NULLS FIRST unspecifiedframe$())\n",
       "lon2: org.apache.spark.sql.Column = longitude\n",
       "lon1: org.apache.spark.sql.Column = lag(longitude, 1, NULL) OVER (ORDER BY timestamp_id ASC NULLS FIRST unspecifiedframe$())\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val lat_col = Window.partitionBy(\"latitude\").orderBy($\"timestamp_id\".asc)\n",
    "// val lon_col = Window.partitionBy(\"longitude\").orderBy($\"timestamp_id\".asc)\n",
    "val lat_col = Window.orderBy($\"timestamp_id\".asc)\n",
    "val lon_col = Window.orderBy($\"timestamp_id\".asc)\n",
    "\n",
    "val lat2 = col(\"latitude\")\n",
    "val lat1 = lag(\"latitude\", 1).over(lat_col)\n",
    "// val lat1 = when((lag(\"latitude\", 1).over(lat_col)).isNotNull, lag(\"latitude\", 1).over(lat_col)).otherwise(0)\n",
    "\n",
    "val lon2 = col(\"longitude\")\n",
    "val lon1 = lag(\"longitude\", 1).over(lat_col)\n",
    "// val lon1 = when((lag(\"longitude\", 1).over(lon_col)).isNotNull, lag(\"longitude\", 1).over(lon_col)).otherwise(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emails: String = viniciusmgaspar@gmail.com\n",
       "joinExpression: org.apache.spark.sql.Column = (uid = uid)\n",
       "joinType: String = inner\n",
       "locationsWithDatePerUserDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [timestamp_id: string, accuracy: double ... 20 more fields]\n",
       "locDatePerUserDistDF: org.apache.spark.sql.DataFrame = [timestamp_id: string, accuracy: double ... 21 more fields]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val emails = \"viniciusmgaspar@gmail.com\"\n",
    "\n",
    "val joinExpression = locationsWithDateDF.col(\"uid\") === usersDF.col(\"uid\")\n",
    "var joinType = \"inner\"\n",
    "val locationsWithDatePerUserDF = locationsWithDateDF.join(usersDF, joinExpression, joinType).drop(usersDF.col(\"uid\")).filter($\"Email\" === emails).orderBy($\"timestamp_id\")\n",
    "\n",
    "val locDatePerUserDistDF = locationsWithDatePerUserDF.withColumn(\"distance\", when(calculate_distance_col(lat1, lon1, lat2, lon2).isNotNull,calculate_distance_col(lat1, lon1, lat2, lon2)).otherwise(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show walked distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------+-----------+-----------+------------------+\n",
      "|username       |ts_date                |latitude   |longitude  |distance          |\n",
      "+---------------+-----------------------+-----------+-----------+------------------+\n",
      "|viniciusmgaspar|2020-10-07 23:32:51.945|-22.9143424|-43.2479267|0.0               |\n",
      "|viniciusmgaspar|2020-10-07 23:34:51.961|-22.9143476|-43.2479225|0.720675492840136 |\n",
      "|viniciusmgaspar|2020-10-07 23:36:56.689|-22.9143736|-43.2479302|2.9967018357873862|\n",
      "|viniciusmgaspar|2020-10-07 23:37:57.042|-22.9143736|-43.2479302|0.0               |\n",
      "|viniciusmgaspar|2020-10-07 23:38:57.116|-22.9143736|-43.2479302|0.0               |\n",
      "|viniciusmgaspar|2020-10-07 23:40:02.011|-22.914351 |-43.247925 |2.5688213633650445|\n",
      "|viniciusmgaspar|2020-10-07 23:41:02.089|-22.914351 |-43.247925 |0.0               |\n",
      "|viniciusmgaspar|2020-10-07 23:42:09.072|-22.9143672|-43.2479284|1.8347079899956038|\n",
      "|viniciusmgaspar|2020-10-07 23:43:09.149|-22.9143672|-43.2479284|0.0               |\n",
      "|viniciusmgaspar|2020-10-07 23:44:10.298|-22.9143608|-43.2479235|0.8708071547226146|\n",
      "|viniciusmgaspar|2020-10-07 23:45:10.37 |-22.9143608|-43.2479235|0.0               |\n",
      "|viniciusmgaspar|2020-10-07 23:46:20.77 |-22.9143608|-43.2479235|0.0               |\n",
      "|viniciusmgaspar|2020-10-07 23:47:20.86 |-22.9143608|-43.2479235|0.0               |\n",
      "|viniciusmgaspar|2020-10-07 23:48:26.977|-22.9143612|-43.2479639|4.138018973396575 |\n",
      "|viniciusmgaspar|2020-10-07 23:49:33.323|-22.914376 |-43.2479024|6.510281292123442 |\n",
      "|viniciusmgaspar|2020-10-07 23:50:33.676|-22.914376 |-43.2479024|0.0               |\n",
      "|viniciusmgaspar|2020-10-07 23:51:33.654|-22.9143854|-43.2478801|2.5117802113336802|\n",
      "|viniciusmgaspar|2020-10-07 23:52:33.892|-22.9143854|-43.2478801|0.0               |\n",
      "|viniciusmgaspar|2020-10-07 23:53:33.964|-22.9143854|-43.2478801|0.0               |\n",
      "|viniciusmgaspar|2020-10-07 23:54:34.786|-22.9143797|-43.2478934|1.5024238063887392|\n",
      "+---------------+-----------------------+-----------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "locDatePerUserDistDF.orderBy($\"ts_date\".asc)\n",
    ".select(\"username\",\"ts_date\",\"latitude\",\"longitude\",\"distance\")\n",
    "    .show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walked distance per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+----------+------------------+\n",
      "|email                    |date      |sum(distance)     |\n",
      "+-------------------------+----------+------------------+\n",
      "|viniciusmgaspar@gmail.com|07-10-2020|27.519468577232455|\n",
      "|viniciusmgaspar@gmail.com|08-10-2020|27236.48613099252 |\n",
      "|viniciusmgaspar@gmail.com|09-10-2020|9797.745650086414 |\n",
      "|viniciusmgaspar@gmail.com|11-10-2020|617.8502728067592 |\n",
      "|viniciusmgaspar@gmail.com|12-10-2020|2232.8849177640095|\n",
      "+-------------------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "locDatePerUserDistDF.groupBy($\"email\",date_format(col(\"ts_date\"),\"dd-MM-yyyy\").as(\"date\")).sum(\"distance\").orderBy($\"email\".asc, $\"date\".asc)\n",
    ".show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of reduceByKey to sum up values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// val userReducedDistance = flatLocationsWithDistDF.filte().filter().reduceByKey((v1,v2) => v1 + v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Between Two People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val user1 = \"henrique.mageste@gmail.com\"\n",
    "val user2 = \"wallace.mendes.rj@gmail.com\"\n",
    "val time_interval_1 = lit(\"2020-10-07 20:20\")\n",
    "val time_interval_2 = lit(\"2020-10-07 20:20\")\n",
    "\n",
    "val user1_DF = locWithDateJoinUserDF.filter($\"email\" === user1).as(\"user1\")\n",
    "val user2_DF = locWithDateJoinUserDF.filter($\"email\" === user2).as(\"user2\")\n",
    "\n",
    "val joinExpression = ( col(\"user1.date\") === user2_DF.col(\"user2.date\") &&  col(\"user1.hours\") === user2_DF.col(\"user2.hours\") && col(\"user1.minutes\") === user2_DF.col(\"user2.minutes\"))\n",
    "val joinType = \"inner\"\n",
    "val c = user1_DF.join(user2_DF, joinExpression, joinType)\n",
    "\n",
    "// mostrar o resultado do join\n",
    "c.select(\n",
    "    col(\"user1.ts_date\")\n",
    "    ,col(\"user2.ts_date\")\n",
    "    ,col(\"user1.hours\")\n",
    "    ,col(\"user2.hours\")\n",
    "    ,col(\"user1.minutes\")\n",
    "    ,col(\"user2.minutes\")\n",
    "    ,col(\"user1.email\")\n",
    "    ,col(\"user2.email\")\n",
    "    ,col(\"user1.longitude\")\n",
    "    ,col(\"user2.longitude\")\n",
    "    ,col(\"user1.latitude\")\n",
    "    ,col(\"user2.latitude\")\n",
    "    ,col(\"user1.address\")\n",
    "    ,col(\"user2.address\")\n",
    "    )\n",
    "    .withColumn(\"distance_between\", calculate_distance_col(col(\"user1.latitude\"), col(\"user1.longitude\"), col(\"user2.latitude\"), col(\"user2.longitude\")) )\n",
    "    \n",
    "val c2= c.select(\n",
    "    col(\"user1.ts_date\")\n",
    "    ,col(\"user2.ts_date\")\n",
    "    ,col(\"user1.hours\")\n",
    "    ,col(\"user2.hours\")\n",
    "    ,col(\"user1.minutes\")\n",
    "    ,col(\"user2.minutes\")\n",
    "    ,col(\"user1.email\")\n",
    "    ,col(\"user2.email\")\n",
    "    ,col(\"user1.longitude\")\n",
    "    ,col(\"user2.longitude\")\n",
    "    ,col(\"user1.latitude\")\n",
    "    ,col(\"user2.latitude\")\n",
    "    ,col(\"user1.address\")\n",
    "    ,col(\"user2.address\")\n",
    "    )\n",
    "    .withColumn(\"distance_between\", calculate_distance_col(col(\"user1.latitude\"), col(\"user1.longitude\"), col(\"user2.latitude\"), col(\"user2.longitude\")) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// AGREGADO POR DATA\n",
    "c2.groupBy(col(\"user1.ts_date\"),col(\"user2.ts_date\")).agg(min(\"distance_between\"),max(\"distance_between\")).show()\n",
    "\n",
    "// AGREGADO POR USUÁRIO    \n",
    "c2.groupBy(col(\"user1.email\"),col(\"user2.email\")).agg(min(\"distance_between\"),max(\"distance_between\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val user1 = \"henrique.mageste@gmail.com\"\n",
    "val user2 = \"viniciusmgaspar@gmail.com\"\n",
    "val time_interval_1 = lit(\"2020-10-07 20:20\")\n",
    "val time_interval_2 = lit(\"2020-10-07 20:20\")\n",
    "\n",
    "var joinType = \"inner\"\n",
    "val joinExpression = locationsWithDateDF.col(\"uid\") === usersDF.col(\"uid\")\n",
    "\n",
    "val user1_DF = locationsWithDateDF.join(usersDF, joinExpression, joinType).drop(usersDF.col(\"uid\")).filter($\"Email\" === user1).orderBy($\"timestamp_id\")\n",
    "val user2_DF = locationsWithDateDF.join(usersDF, joinExpression, joinType).drop(usersDF.col(\"uid\")).filter($\"Email\" === user2).orderBy($\"timestamp_id\")\n",
    "\n",
    "val user1_time_interval = user1_DF.filter($\"ts_date\" >= time_interval_1).filter($\"ts_date\" <= time_interval_2)\n",
    "val user2_time_interval = user2_DF.filter($\"ts_date\" >= time_interval_1).filter($\"ts_date\" <= time_interval_2)\n",
    "\n",
    "user1_time_interval.show()\n",
    "// val user1_lat = user1_time_interval.select(\"latitude\").first()\n",
    "// val user1_lon = user1_time_interval.select(\"longitude\").first()\n",
    "\n",
    "// val user2_lat = user2_time_interval.select(\"latitude\").first()\n",
    "// val user2_lon = user2_time_interval.select(\"longitude\").first()\n",
    "\n",
    "// val d = calculate_distance_elem(user1_lat.getDouble(0), user1_lon.getDouble(0), user2_lat.getDouble(0), user2_lon.getDouble(0))\n",
    "\n",
    "// println(\"distance between user1 and user2: \" + d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For writing SQL syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+--------------------+------------------+-------+-----------+-----------+--------+----+---+-----+-------+-----+---------+-------+-------------+--------------+----+--------------------+--------------------+--------------------+---------------+------------------+\n",
      "| timestamp_id|          accuracy|             address|          altitude|country|   latitude|  longitude|provider|date|day|hours|minutes|month|    nanos|seconds|         time|timezoneOffset|year|                 uid|             ts_date|               email|       username|          distance|\n",
      "+-------------+------------------+--------------------+------------------+-------+-----------+-----------+--------+----+---+-----+-------+-----+---------+-------+-------------+--------------+----+--------------------+--------------------+--------------------+---------------+------------------+\n",
      "|1602124371945|14.199000358581543|R. Tôrres Homem, ...|19.200000762939453| Brasil|-22.9143424|-43.2479267|   fused|   7|  3|   23|     32|    9|945000000|     51|1602124371945|           180| 120|PgXtDvjeJQgc6FzN9...|2020-10-07 23:32:...|viniciusmgaspar@g...|viniciusmgaspar|               0.0|\n",
      "|1602124491961|13.723999977111816|R. Tôrres Homem, ...|              17.0| Brasil|-22.9143476|-43.2479225|   fused|   7|  3|   23|     34|    9|961000000|     51|1602124491961|           180| 120|PgXtDvjeJQgc6FzN9...|2020-10-07 23:34:...|viniciusmgaspar@g...|viniciusmgaspar| 0.720675492840136|\n",
      "|1602124616689|14.432000160217285|R. Tôrres Homem, ...|19.200000762939453| Brasil|-22.9143736|-43.2479302|   fused|   7|  3|   23|     36|    9|689000000|     56|1602124616689|           180| 120|PgXtDvjeJQgc6FzN9...|2020-10-07 23:36:...|viniciusmgaspar@g...|viniciusmgaspar|2.9967018357873862|\n",
      "|1602124677042|14.432000160217285|R. Tôrres Homem, ...|19.200000762939453| Brasil|-22.9143736|-43.2479302|   fused|   7|  3|   23|     37|    9| 42000000|     57|1602124677042|           180| 120|PgXtDvjeJQgc6FzN9...|2020-10-07 23:37:...|viniciusmgaspar@g...|viniciusmgaspar|               0.0|\n",
      "|1602124737116|14.432000160217285|R. Tôrres Homem, ...|19.200000762939453| Brasil|-22.9143736|-43.2479302|   fused|   7|  3|   23|     38|    9|116000000|     57|1602124737116|           180| 120|PgXtDvjeJQgc6FzN9...|2020-10-07 23:38:...|viniciusmgaspar@g...|viniciusmgaspar|               0.0|\n",
      "|1602124802011| 15.61299991607666|R. Tôrres Homem, ...|27.100000381469727| Brasil| -22.914351| -43.247925|   fused|   7|  3|   23|     40|    9| 11000000|      2|1602124802011|           180| 120|PgXtDvjeJQgc6FzN9...|2020-10-07 23:40:...|viniciusmgaspar@g...|viniciusmgaspar|2.5688213633650445|\n",
      "|1602124862089| 15.61299991607666|R. Tôrres Homem, ...|27.100000381469727| Brasil| -22.914351| -43.247925|   fused|   7|  3|   23|     41|    9| 89000000|      2|1602124862089|           180| 120|PgXtDvjeJQgc6FzN9...|2020-10-07 23:41:...|viniciusmgaspar@g...|viniciusmgaspar|               0.0|\n",
      "|1602124929072| 14.53600025177002|R. Tôrres Homem, ...|19.200000762939453| Brasil|-22.9143672|-43.2479284|   fused|   7|  3|   23|     42|    9| 72000000|      9|1602124929072|           180| 120|PgXtDvjeJQgc6FzN9...|2020-10-07 23:42:...|viniciusmgaspar@g...|viniciusmgaspar|1.8347079899956038|\n",
      "|1602124989149| 14.53600025177002|R. Tôrres Homem, ...|19.200000762939453| Brasil|-22.9143672|-43.2479284|   fused|   7|  3|   23|     43|    9|149000000|      9|1602124989149|           180| 120|PgXtDvjeJQgc6FzN9...|2020-10-07 23:43:...|viniciusmgaspar@g...|viniciusmgaspar|               0.0|\n",
      "|1602125050298|15.727999687194824|R. Tôrres Homem, ...|27.100000381469727| Brasil|-22.9143608|-43.2479235|   fused|   7|  3|   23|     44|    9|298000000|     10|1602125050298|           180| 120|PgXtDvjeJQgc6FzN9...|2020-10-07 23:44:...|viniciusmgaspar@g...|viniciusmgaspar|0.8708071547226146|\n",
      "|1602125110370|15.727999687194824|R. Tôrres Homem, ...|27.100000381469727| Brasil|-22.9143608|-43.2479235|   fused|   7|  3|   23|     45|    9|370000000|     10|1602125110370|           180| 120|PgXtDvjeJQgc6FzN9...|2020-10-07 23:45:...|viniciusmgaspar@g...|viniciusmgaspar|               0.0|\n",
      "|1602125180770|15.727999687194824|R. Tôrres Homem, ...|27.100000381469727| Brasil|-22.9143608|-43.2479235|   fused|   7|  3|   23|     46|    9|770000000|     20|1602125180770|           180| 120|PgXtDvjeJQgc6FzN9...|2020-10-07 23:46:...|viniciusmgaspar@g...|viniciusmgaspar|               0.0|\n",
      "|1602125240860|15.727999687194824|R. Tôrres Homem, ...|27.100000381469727| Brasil|-22.9143608|-43.2479235|   fused|   7|  3|   23|     47|    9|860000000|     20|1602125240860|           180| 120|PgXtDvjeJQgc6FzN9...|2020-10-07 23:47:...|viniciusmgaspar@g...|viniciusmgaspar|               0.0|\n",
      "|1602125306977|14.163999557495117|R. Tôrres Homem, ...|              17.0| Brasil|-22.9143612|-43.2479639|   fused|   7|  3|   23|     48|    9|977000000|     26|1602125306977|           180| 120|PgXtDvjeJQgc6FzN9...|2020-10-07 23:48:...|viniciusmgaspar@g...|viniciusmgaspar| 4.138018973396575|\n",
      "|1602125373323|15.175999641418457|R. Tôrres Homem, ...| 19.80000114440918| Brasil| -22.914376|-43.2479024|   fused|   7|  3|   23|     49|    9|323000000|     33|1602125373323|           180| 120|PgXtDvjeJQgc6FzN9...|2020-10-07 23:49:...|viniciusmgaspar@g...|viniciusmgaspar| 6.510281292123442|\n",
      "|1602125433676|15.175999641418457|R. Tôrres Homem, ...| 19.80000114440918| Brasil| -22.914376|-43.2479024|   fused|   7|  3|   23|     50|    9|676000000|     33|1602125433676|           180| 120|PgXtDvjeJQgc6FzN9...|2020-10-07 23:50:...|viniciusmgaspar@g...|viniciusmgaspar|               0.0|\n",
      "|1602125493654|17.315000534057617|R. Tôrres Homem, ...|27.100000381469727| Brasil|-22.9143854|-43.2478801|   fused|   7|  3|   23|     51|    9|654000000|     33|1602125493654|           180| 120|PgXtDvjeJQgc6FzN9...|2020-10-07 23:51:...|viniciusmgaspar@g...|viniciusmgaspar|2.5117802113336802|\n",
      "|1602125553892|17.315000534057617|R. Tôrres Homem, ...|27.100000381469727| Brasil|-22.9143854|-43.2478801|   fused|   7|  3|   23|     52|    9|892000000|     33|1602125553892|           180| 120|PgXtDvjeJQgc6FzN9...|2020-10-07 23:52:...|viniciusmgaspar@g...|viniciusmgaspar|               0.0|\n",
      "|1602125613964|17.315000534057617|R. Tôrres Homem, ...|27.100000381469727| Brasil|-22.9143854|-43.2478801|   fused|   7|  3|   23|     53|    9|964000000|     33|1602125613964|           180| 120|PgXtDvjeJQgc6FzN9...|2020-10-07 23:53:...|viniciusmgaspar@g...|viniciusmgaspar|               0.0|\n",
      "|1602125674786|15.434000015258789|R. Tôrres Homem, ...|27.100000381469727| Brasil|-22.9143797|-43.2478934|   fused|   7|  3|   23|     54|    9|786000000|     34|1602125674786|           180| 120|PgXtDvjeJQgc6FzN9...|2020-10-07 23:54:...|viniciusmgaspar@g...|viniciusmgaspar|1.5024238063887392|\n",
      "+-------------+------------------+--------------------+------------------+-------+-----------+-----------+--------+----+---+-----+-------+-----+---------+-------+-------------+--------------+----+--------------------+--------------------+--------------------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sqlDF: org.apache.spark.sql.DataFrame = [timestamp_id: string, accuracy: double ... 21 more fields]\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Register the DataFrame as a SQL temporary view\n",
    "locDatePerUserDistDF.createOrReplaceTempView(\"locationsSQL\")\n",
    "\n",
    "val sqlDF = spark.sql(\"SELECT * FROM locationsSQL\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "16: error: Unable to find encoder for type UserLocation. An implicit Encoder[UserLocation] is needed to store UserLocation instances in a Dataset. Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._  Support for serializing other types will be added in future releases.",
     "output_type": "error",
     "traceback": [
      "<console>:16: error: Unable to find encoder for type UserLocation. An implicit Encoder[UserLocation] is needed to store UserLocation instances in a Dataset. Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._  Support for serializing other types will be added in future releases.",
      "       val locWithDateJoinUserDS = locWithDateJoinUserDF.select(\"email\", \"latitude\", \"longitude\", \"distance\").as[UserLocation]",
      "                                                                                                                ^",
      ""
     ]
    }
   ],
   "source": [
    "case class UserLocation(email: String, latitude: Double, longitude: Double, distance: Double)\n",
    "\n",
    "val locWithDateJoinUserDS = locWithDateJoinUserDF.select(\"email\", \"latitude\", \"longitude\", \"distance\").as[UserLocation]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
