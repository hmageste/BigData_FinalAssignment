{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data - Final Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial configuration for Spark + JVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master = \"local[*]\"\n",
    "launcher.driver_memory = '20g'\n",
    "launcher.executor_memory = '20g'\n",
    "launcher.verbose = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.conf.set(\"spark.app.name\", \"scalaXgbTest\")\n",
    "launcher.num_executors = 3\n",
    "launcher.executor_cores = 7 //launcher.conf.spark.executor.cores = 8\n",
    "launcher.conf.spark.task.cpus = 6\n",
    "launcher.driver_memory = '4g'\n",
    "launcher.executor_memory = '4g'\n",
    "launcher.conf.set(\"spark.executor.heartbeatInterval\", \"6000s\")\n",
    "launcher.conf.set(\"spark.yarn.scheduler.heartbeat.interval-ms\", \"10000s\")\n",
    "launcher.conf.set(\"spark.network.timeout\", \"10000s\")\n",
    "launcher.conf.set(\"spark.yarn.executor.memoryOverhead\", \"8192\")\n",
    "launcher.conf.set(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "launcher.jars = [\"file://some/jar.jar\", \"xgboost-maven-0.82/xgboost4j-spark-0.82.jar\", \"xgboost-maven-0.82/xgboost4j-0.82.jar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://cln-rio-06:4040\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1602432217912)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spylon-kernel\n",
      "local[*]\n"
     ]
    }
   ],
   "source": [
    "println(sc.appName)\n",
    "println(sc.master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import spark.implicits._\n",
       "import org.apache.spark.sql.expressions._\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._      // include the Spark Types to define our schema\n",
    "import org.apache.spark.sql.functions._  // include the Spark helper functions\n",
    "import spark.implicits._                 // For implicit conversions like converting RDDs to DataFrames\n",
    "import org.apache.spark.sql.expressions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB JSON schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location_schema: org.apache.spark.sql.types.MapType = MapType(StringType,StructType(StructField(accuracy,DoubleType,true), StructField(address,StringType,true), StructField(altitude,DoubleType,true), StructField(country,StringType,true), StructField(latitude,DoubleType,true), StructField(longitude,DoubleType,true), StructField(provider,StringType,true), StructField(timestamp,StructType(StructField(date,LongType,true), StructField(day,LongType,true), StructField(hours,LongType,true), StructField(month,LongType,true), StructField(nanos,LongType,true), StructField(seconds,LongType,true), StructField(time,LongType,true), StructField(timezoneOffset,LongType,true), StructField(year,LongType,true)),true), StructField(uid,StringType,true)),true)\n",
       "schema: org.apache.spark.sql.types.StructType = S...\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val location_schema =\n",
    "    MapType(StringType,\n",
    "        new StructType()\n",
    "            .add(\"accuracy\", DoubleType)\n",
    "            .add(\"address\", StringType)\n",
    "            .add(\"altitude\", DoubleType)\n",
    "            .add(\"country\", StringType)\n",
    "            .add(\"latitude\", DoubleType)\n",
    "            .add(\"longitude\", DoubleType)\n",
    "            .add(\"provider\", StringType)\n",
    "            .add(\"timestamp\", \n",
    "             new StructType()\n",
    "                .add(\"date\", LongType)\n",
    "                .add(\"day\", LongType)\n",
    "                .add(\"hours\", LongType)\n",
    "                .add(\"month\", LongType)\n",
    "                .add(\"nanos\", LongType)\n",
    "                .add(\"seconds\", LongType)\n",
    "                .add(\"time\", LongType)\n",
    "                .add(\"timezoneOffset\", LongType)\n",
    "                .add(\"year\", LongType)\n",
    "            )\n",
    "            .add(\"uid\", StringType)\n",
    "        )\n",
    "\n",
    "val schema = new StructType()\n",
    "    .add(\"locations\", location_schema)\n",
    "    .add(\"user-locations\",\n",
    "        MapType(StringType, location_schema)\n",
    "    )\n",
    "    .add(\"users\",\n",
    "        MapType(StringType,\n",
    "            new StructType()\n",
    "                .add(\"email\", StringType)\n",
    "                .add(\"username\", StringType)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [locations: map<string,struct<accuracy:double,address:string,altitude:double,country:string,latitude:double,longitude:double,provider:string,timestamp:struct<date:bigint,day:bigint,hours:bigint,month:bigint,nanos:bigint,seconds:bigint,time:bigint,timezoneOffset:bigint,year:bigint>,uid:string>>, user-locations: map<string,map<string,struct<accuracy:double,address:string,altitude:double,country:string,latitude:double,longitude:double,provider:string,timestamp:struct<date:bigint,day:bigint,hours:bigint,month:bigint,nanos:bigint,seconds:bigint,time:bigint,timezoneOffset:bigint,year:bigint>,uid:string>>> ... 1 more field]\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.option(\"multiline\", true).schema(schema).json(\"trackme-sample-data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- locations: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: struct (valueContainsNull = true)\n",
      " |    |    |-- accuracy: double (nullable = true)\n",
      " |    |    |-- address: string (nullable = true)\n",
      " |    |    |-- altitude: double (nullable = true)\n",
      " |    |    |-- country: string (nullable = true)\n",
      " |    |    |-- latitude: double (nullable = true)\n",
      " |    |    |-- longitude: double (nullable = true)\n",
      " |    |    |-- provider: string (nullable = true)\n",
      " |    |    |-- timestamp: struct (nullable = true)\n",
      " |    |    |    |-- date: long (nullable = true)\n",
      " |    |    |    |-- day: long (nullable = true)\n",
      " |    |    |    |-- hours: long (nullable = true)\n",
      " |    |    |    |-- month: long (nullable = true)\n",
      " |    |    |    |-- nanos: long (nullable = true)\n",
      " |    |    |    |-- seconds: long (nullable = true)\n",
      " |    |    |    |-- time: long (nullable = true)\n",
      " |    |    |    |-- timezoneOffset: long (nullable = true)\n",
      " |    |    |    |-- year: long (nullable = true)\n",
      " |    |    |-- uid: string (nullable = true)\n",
      " |-- user-locations: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: map (valueContainsNull = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: struct (valueContainsNull = true)\n",
      " |    |    |    |-- accuracy: double (nullable = true)\n",
      " |    |    |    |-- address: string (nullable = true)\n",
      " |    |    |    |-- altitude: double (nullable = true)\n",
      " |    |    |    |-- country: string (nullable = true)\n",
      " |    |    |    |-- latitude: double (nullable = true)\n",
      " |    |    |    |-- longitude: double (nullable = true)\n",
      " |    |    |    |-- provider: string (nullable = true)\n",
      " |    |    |    |-- timestamp: struct (nullable = true)\n",
      " |    |    |    |    |-- date: long (nullable = true)\n",
      " |    |    |    |    |-- day: long (nullable = true)\n",
      " |    |    |    |    |-- hours: long (nullable = true)\n",
      " |    |    |    |    |-- month: long (nullable = true)\n",
      " |    |    |    |    |-- nanos: long (nullable = true)\n",
      " |    |    |    |    |-- seconds: long (nullable = true)\n",
      " |    |    |    |    |-- time: long (nullable = true)\n",
      " |    |    |    |    |-- timezoneOffset: long (nullable = true)\n",
      " |    |    |    |    |-- year: long (nullable = true)\n",
      " |    |    |    |-- uid: string (nullable = true)\n",
      " |-- users: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: struct (valueContainsNull = true)\n",
      " |    |    |-- email: string (nullable = true)\n",
      " |    |    |-- username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "locationsDF: org.apache.spark.sql.DataFrame = [timestamp_id: string, value: struct<accuracy: double, address: string ... 7 more fields>]\n",
       "userLocationsDF: org.apache.spark.sql.DataFrame = [uid: string, timestamp: map<string,struct<accuracy:double,address:string,altitude:double,country:string,latitude:double,longitude:double,provider:string,timestamp:struct<date:bigint,day:bigint,hours:bigint,month:bigint,nanos:bigint,seconds:bigint,time:bigint,timezoneOffset:bigint,year:bigint>,uid:string>>]\n",
       "usersDF: org.apache.spark.sql.DataFrame = [uid: string, user_attr: struct<email: string, username: string>]\n"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val locationsDF = df.select(explode($\"locations\") as Seq(\"timestamp_id\", \"value\"))\n",
    "val userLocationsDF = df.select(explode($\"user-locations\") as Seq(\"uid\", \"timestamp\"))\n",
    "val usersDF = df.select(explode($\"users\") as Seq(\"uid\", \"user_attr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = false)\n",
      " |-- user_attr: struct (nullable = true)\n",
      " |    |-- email: string (nullable = true)\n",
      " |    |-- username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to flatten schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Column\n",
       "flattenSchema: (schema: org.apache.spark.sql.types.StructType, prefix: String)Array[org.apache.spark.sql.Column]\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Column\n",
    "\n",
    "def flattenSchema(schema: StructType, prefix: String = null) : Array[Column] = {\n",
    "  schema.fields.flatMap(f => {\n",
    "    val colName = if (prefix == null) f.name else (prefix + \".\" + f.name)\n",
    "\n",
    "    f.dataType match {\n",
    "      case st: StructType => flattenSchema(st, colName)\n",
    "      case _ => Array(col(colName))\n",
    "    }\n",
    "  })\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flat struct DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flatLocationsDF: org.apache.spark.sql.DataFrame = [timestamp_id: string, accuracy: double ... 16 more fields]\n",
       "flatUserLocationsDF: org.apache.spark.sql.DataFrame = [uid: string, timestamp: map<string,struct<accuracy:double,address:string,altitude:double,country:string,latitude:double,longitude:double,provider:string,timestamp:struct<date:bigint,day:bigint,hours:bigint,month:bigint,nanos:bigint,seconds:bigint,time:bigint,timezoneOffset:bigint,year:bigint>,uid:string>>]\n",
       "flatUsersDF: org.apache.spark.sql.DataFrame = [uid: string, email: string ... 1 more field]\n"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flatLocationsDF = locationsDF.select(flattenSchema(locationsDF.schema):_*)\n",
    "val flatUserLocationsDF = userLocationsDF.select(flattenSchema(userLocationsDF.schema):_*)\n",
    "val flatUsersDF = usersDF.select(flattenSchema(usersDF.schema):_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp_id: string (nullable = false)\n",
      " |-- accuracy: double (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- altitude: double (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- provider: string (nullable = true)\n",
      " |-- date: long (nullable = true)\n",
      " |-- day: long (nullable = true)\n",
      " |-- hours: long (nullable = true)\n",
      " |-- month: long (nullable = true)\n",
      " |-- nanos: long (nullable = true)\n",
      " |-- seconds: long (nullable = true)\n",
      " |-- time: long (nullable = true)\n",
      " |-- timezoneOffset: long (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flatLocationsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+-----------------------------------------------------------------------+------------------+-------+-----------+-----------+--------+----+---+-----+-----+---------+-------+-------------+--------------+----+----------------------------+\n",
      "|timestamp_id |accuracy          |address                                                                |altitude          |country|latitude   |longitude  |provider|date|day|hours|month|nanos    |seconds|time         |timezoneOffset|year|uid                         |\n",
      "+-------------+------------------+-----------------------------------------------------------------------+------------------+-------+-----------+-----------+--------+----+---+-----+-----+---------+-------+-------------+--------------+----+----------------------------+\n",
      "|1602119776824|15.666000366210938|R. Mena Barreto, 182 - Botafogo, Rio de Janeiro - RJ, 22271-100, Brazil|14.90000057220459 |Brazil |-22.9556468|-43.1880249|fused   |7   |3  |22   |9    |824000000|16     |1602119776824|180           |120 |H5LG3vN3jcPlcbJ2A5RGo6H4AHw2|\n",
      "|1602119934507|15.967000007629395|R. Mena Barreto, 182 - Botafogo, Rio de Janeiro - RJ, 22271-100, Brazil|15.300000190734863|Brazil |-22.9556367|-43.1880211|fused   |7   |3  |22   |9    |507000000|54     |1602119934507|180           |120 |H5LG3vN3jcPlcbJ2A5RGo6H4AHw2|\n",
      "|1602120846895|22.099000930786133|R. Mena Barreto, 182 - Botafogo, Rio de Janeiro - RJ, 22271-100, Brazil|16.799999237060547|Brazil |-22.9556134|-43.1879997|fused   |7   |3  |22   |9    |895000000|6      |1602120846895|180           |120 |H5LG3vN3jcPlcbJ2A5RGo6H4AHw2|\n",
      "|1602121213639|15.10200023651123 |R. Mena Barreto, 182 - Botafogo, Rio de Janeiro - RJ, 22271-100, Brazil|14.90000057220459 |Brazil |-22.9556477|-43.1880597|fused   |7   |3  |22   |9    |639000000|13     |1602121213639|180           |120 |H5LG3vN3jcPlcbJ2A5RGo6H4AHw2|\n",
      "|1602121277958|13.876999855041504|R. Srg. Aquino, 276 - Olaria, Rio de Janeiro - RJ, 21021-640, Brasil   |2.700000047683716 |Brasil |-22.8381181|-43.2623295|fused   |7   |3  |22   |9    |958000000|17     |1602121277958|180           |120 |uF9x6cGOL9bclGCkkV2PkrcwnIz1|\n",
      "|1602121409097|15.027999877929688|R. Srg. Aquino, 276 - Olaria, Rio de Janeiro - RJ, 21021-640, Brasil   |2.700000047683716 |Brasil |-22.8381179|-43.2623138|fused   |7   |3  |22   |9    |97000000 |29     |1602121409097|180           |120 |uF9x6cGOL9bclGCkkV2PkrcwnIz1|\n",
      "|1602121530180|14.98799991607666 |R. Srg. Aquino, 276 - Olaria, Rio de Janeiro - RJ, 21021-640, Brasil   |2.700000047683716 |Brasil |-22.838109 |-43.2623071|fused   |7   |3  |22   |9    |180000000|30     |1602121530180|180           |120 |uF9x6cGOL9bclGCkkV2PkrcwnIz1|\n",
      "|1602121655540|13.994999885559082|R. Srg. Aquino, 276 - Olaria, Rio de Janeiro - RJ, 21021-640, Brasil   |2.4000000953674316|Brasil |-22.8381141|-43.2623158|fused   |7   |3  |22   |9    |540000000|35     |1602121655540|180           |120 |uF9x6cGOL9bclGCkkV2PkrcwnIz1|\n",
      "|1602121736870|14.697999954223633|R. Mena Barreto, 182 - Botafogo, Rio de Janeiro - RJ, 22271-100, Brazil|8.699999809265137 |Brazil |-22.9556593|-43.1880598|fused   |7   |3  |22   |9    |870000000|56     |1602121736870|180           |120 |H5LG3vN3jcPlcbJ2A5RGo6H4AHw2|\n",
      "|1602121751000|19.0              |R. Srg. Aquino, 276 - Olaria, Rio de Janeiro - RJ, 21021-640, Brasil   |2.700000047683716 |Brasil |-22.837858 |-43.2621474|fused   |7   |3  |22   |9    |0        |11     |1602121751000|180           |120 |uF9x6cGOL9bclGCkkV2PkrcwnIz1|\n",
      "|1602121773593|13.946999549865723|R. Mena Barreto, 182 - Botafogo, Rio de Janeiro - RJ, 22271-100, Brazil|8.699999809265137 |Brazil |-22.9556522|-43.1880422|fused   |7   |3  |22   |9    |593000000|33     |1602121773593|180           |120 |H5LG3vN3jcPlcbJ2A5RGo6H4AHw2|\n",
      "|1602121834763|14.918000221252441|R. Mena Barreto, 182 - Botafogo, Rio de Janeiro - RJ, 22271-100, Brazil|8.699999809265137 |Brazil |-22.9556413|-43.1880571|fused   |7   |3  |22   |9    |763000000|34     |1602121834763|180           |120 |H5LG3vN3jcPlcbJ2A5RGo6H4AHw2|\n",
      "|1602121840113|1100.0            |Av. Brasil, 9200 - Penha, Rio de Janeiro - RJ, 21012-350, Brasil       |27.0              |Brasil |-22.8339884|-43.259253 |fused   |7   |3  |22   |9    |113000000|40     |1602121840113|180           |120 |uF9x6cGOL9bclGCkkV2PkrcwnIz1|\n",
      "|1602121896253|14.197999954223633|R. Mena Barreto, 182 - Botafogo, Rio de Janeiro - RJ, 22271-100, Brazil|8.699999809265137 |Brazil |-22.9556438|-43.1880622|fused   |7   |3  |22   |9    |253000000|36     |1602121896253|180           |120 |H5LG3vN3jcPlcbJ2A5RGo6H4AHw2|\n",
      "|1602121935763|39.430999755859375|R. Srg. Aquino, 276 - Olaria, Rio de Janeiro - RJ, 21021-640, Brasil   |2.700000047683716 |Brasil |-22.8380786|-43.2621135|fused   |7   |3  |22   |9    |763000000|15     |1602121935763|180           |120 |uF9x6cGOL9bclGCkkV2PkrcwnIz1|\n",
      "|1602121957435|14.581999778747559|R. Mena Barreto, 182 - Botafogo, Rio de Janeiro - RJ, 22271-100, Brazil|14.90000057220459 |Brazil |-22.9555958|-43.1880177|fused   |7   |3  |22   |9    |435000000|37     |1602121957435|180           |120 |H5LG3vN3jcPlcbJ2A5RGo6H4AHw2|\n",
      "|1602121987617|12.493000030517578|R. Srg. Aquino, 276 - Olaria, Rio de Janeiro - RJ, 21021-640, Brasil   |2.700000047683716 |Brasil |-22.8381159|-43.2623215|fused   |7   |3  |22   |9    |617000000|7      |1602121987617|180           |120 |uF9x6cGOL9bclGCkkV2PkrcwnIz1|\n",
      "|1602122047811|14.772000312805176|R. Mena Barreto, 182 - Botafogo, Rio de Janeiro - RJ, 22271-100, Brazil|13.300000190734863|Brazil |-22.9555982|-43.1880122|fused   |7   |3  |22   |9    |811000000|7      |1602122047811|180           |120 |H5LG3vN3jcPlcbJ2A5RGo6H4AHw2|\n",
      "|1602122055711|12.493000030517578|R. Srg. Aquino, 276 - Olaria, Rio de Janeiro - RJ, 21021-640, Brasil   |2.700000047683716 |Brasil |-22.8381159|-43.2623215|fused   |7   |3  |22   |9    |711000000|15     |1602122055711|180           |120 |uF9x6cGOL9bclGCkkV2PkrcwnIz1|\n",
      "|1602122082028|17.5              |R. Srg. Aquino, 348 - Olaria, Rio de Janeiro - RJ, 21021-640, Brasil   |2.4000000953674316|Brasil |-22.8381448|-43.2623323|fused   |7   |3  |22   |9    |28000000 |42     |1602122082028|180           |120 |uF9x6cGOL9bclGCkkV2PkrcwnIz1|\n",
      "+-------------+------------------+-----------------------------------------------------------------------+------------------+-------+-----------+-----------+--------+----+---+-----+-----+---------+-------+-------------+--------------+----+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flatLocationsDF.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Element version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.math._\n",
       "calculate_distance_elem: (lat1: Double, lon1: Double, lat2: Double, lon2: Double)Double\n",
       "calculate_distance_elem_sqlfunc: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$3558/1324939461@3bc06931,DoubleType,List(Some(class[value[0]: double]), Some(class[value[0]: double]), Some(class[value[0]: double]), Some(class[value[0]: double])),None,false,true)\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.math._\n",
    "\n",
    "def calculate_distance_elem(lat1:Double, lon1:Double, lat2:Double, lon2:Double):Double = {   \n",
    "    val earth_radius = 6371e3;           // meters\n",
    "    val phi1 = lat1 * Pi/180;                  // radians\n",
    "    val phi2 = lat2 * Pi/180;                  // radians\n",
    "    val delta_phi = phi2 - phi1;               // radians\n",
    "\n",
    "    val delta_lampda = (lon2 - lon1) * Pi/180; // radians\n",
    "\n",
    "    val a = sin(delta_phi/2)*sin(delta_phi/2) + cos(phi1)*cos(phi2)*sin(delta_lampda/2)*sin(delta_lampda/2);\n",
    "    val c = 2*atan2(sqrt(a), sqrt(1-a));\n",
    "\n",
    "    val d = earth_radius*c; // meters\n",
    "    \n",
    "    return d\n",
    "}\n",
    "\n",
    "val calculate_distance_elem_sqlfunc = udf(calculate_distance_elem(_,_,_,_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n",
       "calculate_distance_col: (lat1: org.apache.spark.sql.Column, lon1: org.apache.spark.sql.Column, lat2: org.apache.spark.sql.Column, lon2: org.apache.spark.sql.Column)org.apache.spark.sql.Column\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//import scala.math._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "def calculate_distance_col(lat1:org.apache.spark.sql.Column, lon1:org.apache.spark.sql.Column, lat2:org.apache.spark.sql.Column, lon2:org.apache.spark.sql.Column):org.apache.spark.sql.Column = {   \n",
    "    val earth_radius = 6371e3;           // meters\n",
    "    val pi_over_180 = lit(Pi/180);\n",
    "    val phi1 = lat1 * pi_over_180;                  // radians\n",
    "    val phi2 = lat2 * pi_over_180;                  // radians\n",
    "    val delta_phi = phi2 - phi1;               // radians\n",
    "\n",
    "    val delta_lampda = (lon2 - lon1) * pi_over_180; // radians\n",
    "\n",
    "    val a = sin(delta_phi/2)*sin(delta_phi/2) + cos(phi1)*cos(phi2)*sin(delta_lampda/2)*sin(delta_lampda/2);\n",
    "    val c = lit(2)*atan2(sqrt(a), sqrt(lit(1)-a));\n",
    "\n",
    "    val d = lit(earth_radius)*c; // meters\n",
    "    \n",
    "    return d;\n",
    "}\n",
    "\n",
    "// val calculate_distance_sqlfunc = udf(calculate_distance(_,_,_,_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test calculate_distance_elem function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lat1: Double = -22.9556473\n",
       "lon1: Double = -43.1881019\n",
       "lat2: Double = -23.9556473\n",
       "lon2: Double = -44.1881019\n",
       "dist: Double = 150894.75616346067\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lat1 = -22.9556473\n",
    "val lon1 = -43.1881019\n",
    "\n",
    "val lat2 = -23.9556473\n",
    "val lon2 = -44.1881019\n",
    "\n",
    "val dist = calculate_distance_elem(lat1, lon1, lat2, lon2)\n",
    "\n",
    "assert (dist == 150894.75616346067)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lat_col: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@a177a6e\n",
       "lon_col: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@7b15c6d0\n",
       "lat2: org.apache.spark.sql.Column = latitude\n",
       "lat1: org.apache.spark.sql.Column = CASE WHEN (lag(latitude, 1, NULL) OVER (PARTITION BY latitude ORDER BY timestamp_id ASC NULLS FIRST unspecifiedframe$()) IS NOT NULL) THEN lag(latitude, 1, NULL) OVER (PARTITION BY latitude ORDER BY timestamp_id ASC NULLS FIRST unspecifiedframe$()) ELSE latitude END\n",
       "lon2: org.apache.spark.sql.Column = longitude\n",
       "lon1: org.apache.spark.sql.Column = CASE WHEN (lag(longitude, 1, NULL) OVER (PARTITION BY longitude ORDER BY timestamp_id ASC NULLS FIRST unspecifiedframe$()) IS NOT NULL) THEN lag(longitu...\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lat_col = Window.partitionBy(\"latitude\").orderBy(\"timestamp_id\")\n",
    "val lon_col = Window.partitionBy(\"longitude\").orderBy(\"timestamp_id\")\n",
    "\n",
    "val lat2 = col(\"latitude\")\n",
    "// val lat1 = lag(\"latitude\", 1).over(lat_col)\n",
    "val lat1 = when((lag(\"latitude\", 1).over(lat_col)).isNotNull, lag(\"latitude\", 1).over(lat_col)).otherwise($\"latitude\")\n",
    "\n",
    "val lon2 = col(\"longitude\")\n",
    "// val lon1 = lag(\"longitude\", 1).over(lat_col)\n",
    "val lon1 = when((lag(\"longitude\", 1).over(lon_col)).isNotNull, lag(\"longitude\", 1).over(lon_col)).otherwise($\"longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flatLocationsWithDistDF: org.apache.spark.sql.DataFrame = [timestamp_id: bigint, accuracy: double ... 17 more fields]\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flatLocationsWithDistDF = flatLocationsDF.withColumn(\"distance\", calculate_distance_col(lat1, lon1, lat2, lon2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp_id: long (nullable = false)\n",
      " |-- accuracy: double (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- altitude: double (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- provider: string (nullable = true)\n",
      " |-- date: long (nullable = true)\n",
      " |-- day: long (nullable = true)\n",
      " |-- hours: long (nullable = true)\n",
      " |-- month: long (nullable = true)\n",
      " |-- nanos: long (nullable = true)\n",
      " |-- seconds: long (nullable = true)\n",
      " |-- time: long (nullable = true)\n",
      " |-- timezoneOffset: long (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- distance: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flatLocationsWithDistDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 9, cln-rio-06, executor driver): java.lang.ClassCastException",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 9, cln-rio-06, executor driver): java.lang.ClassCastException",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2697)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2904)",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:337)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:826)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:803)",
      "  ... 56 elided",
      "Caused by: java.lang.ClassCastException",
      ""
     ]
    }
   ],
   "source": [
    "flatLocationsWithDistDF\n",
    ".select(\"distance\")\n",
    "    .show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "66: error: not found: value join",
     "output_type": "error",
     "traceback": [
      "<console>:66: error: not found: value join",
      "       val consultaFinal = join.filter($\"Email\" === emails)",
      "                           ^",
      ""
     ]
    }
   ],
   "source": [
    "val emails = \"wallace.mendes.rj@gmail.com\"\n",
    "val consultaFinal = join.filter($\"Email\" === emails)\n",
    "val windowSpec = Window.orderBy(\"Timestamp\")\n",
    "val testeDF = consultaFinal.select(\"Email\",\"Timestamp\",\"Longitude\").withColumn(\"distancia\", $\"Longitude\" - when((lag(\"Longitude\", 1).over(windowSpec)).isNotNull, lag(\"Longitude\", 1).over(windowSpec)).otherwise($\"Longitude\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
